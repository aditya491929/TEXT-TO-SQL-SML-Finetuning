{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip -q install transformers peft accelerate bitsandbytes datasets sqlglot tqdm huggingface_hub"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, json, re, sqlite3, random\n",
    "from pathlib import Path\n",
    "from typing import Dict\n",
    "import pandas as pd\n",
    "from datasets import Dataset, DatasetDict, load_dataset\n",
    "from tqdm import tqdm\n",
    "import sqlglot\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6da4db0b60d24ec8a591a4136498d0aa",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(HTML(value='<center> <img\\nsrc=https://huggingface.co/front/assets/huggingface_logo-noborder.svâ€¦"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Token has not been saved to git credential helper.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from huggingface_hub import login\n",
    "\n",
    "login()  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['db_id', 'question', 'prompt', 'sql_gold'],\n",
       "        num_rows: 7000\n",
       "    })\n",
       "    dev: Dataset({\n",
       "        features: ['db_id', 'question', 'prompt', 'sql_gold'],\n",
       "        num_rows: 1034\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "PREPARED_DIR = Path(\"./prepared\") \n",
    "DATA_FILES = {\n",
    "    \"train\": str(PREPARED_DIR/\"spider_train.jsonl\"),\n",
    "    \"dev\":   str(PREPARED_DIR/\"spider_dev.jsonl\"),\n",
    "}\n",
    "\n",
    "ds = load_dataset(\"json\", data_files=DATA_FILES)\n",
    "ds\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using: meta-llama/Llama-3.2-3B\n"
     ]
    }
   ],
   "source": [
    "\n",
    "BASE_MODEL = \"meta-llama/Llama-3.2-3B\"\n",
    "\n",
    "print(\"Using:\", BASE_MODEL)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(BASE_MODEL, use_fast=True)\n",
    "if tokenizer.pad_token is None:\n",
    "    tokenizer.pad_token = tokenizer.eos_token\n",
    "tokenizer.padding_side = \"right\"\n",
    "\n",
    "MAX_LEN = 2048  # drop to 1536 if you hit OOM\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6bc7f7bd40244739b4aca425108e1700",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/7000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "939a30b8b0ce425d924366c5beccd5df",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/1034 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['input_ids', 'attention_mask', 'labels'],\n",
       "        num_rows: 7000\n",
       "    })\n",
       "    dev: Dataset({\n",
       "        features: ['input_ids', 'attention_mask', 'labels'],\n",
       "        num_rows: 1034\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def map_to_features(ex):\n",
    "    full   = ex[\"prompt\"] + ex[\"sql_gold\"]\n",
    "    tok_f  = tokenizer(full, truncation=True, max_length=MAX_LEN)\n",
    "    tok_p  = tokenizer(ex[\"prompt\"], truncation=True, max_length=MAX_LEN)\n",
    "    n_prompt = len(tok_p[\"input_ids\"])\n",
    "\n",
    "    labels = tok_f[\"input_ids\"][:]\n",
    "    labels[:n_prompt] = [-100]*n_prompt   # mask prompt tokens\n",
    "\n",
    "    ex[\"input_ids\"] = tok_f[\"input_ids\"]\n",
    "    ex[\"attention_mask\"] = tok_f[\"attention_mask\"]\n",
    "    ex[\"labels\"] = labels\n",
    "    return ex\n",
    "\n",
    "tok_ds = DatasetDict({\n",
    "    \"train\": ds[\"train\"].map(map_to_features, remove_columns=ds[\"train\"].column_names),\n",
    "    \"dev\":   ds[\"dev\"].map(map_to_features,   remove_columns=ds[\"dev\"].column_names),\n",
    "})\n",
    "tok_ds\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The `load_in_4bit` and `load_in_8bit` arguments are deprecated and will be removed in the future versions. Please, pass a `BitsAndBytesConfig` object in `quantization_config` argument instead.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d9b187b77232447cac6ac3657a4680dd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trainable params: 24313856\n"
     ]
    }
   ],
   "source": [
    "import torch, bitsandbytes as bnb\n",
    "from transformers import AutoModelForCausalLM\n",
    "from peft import LoraConfig, get_peft_model, TaskType, prepare_model_for_kbit_training\n",
    "\n",
    "device_map = \"auto\"\n",
    "\n",
    "base = AutoModelForCausalLM.from_pretrained(\n",
    "    BASE_MODEL,\n",
    "    torch_dtype=\"auto\",\n",
    "    load_in_4bit=True,          # QLoRA\n",
    "    device_map=device_map,\n",
    "    trust_remote_code=False\n",
    ")\n",
    "\n",
    "base = prepare_model_for_kbit_training(base)\n",
    "peft_cfg = LoraConfig(\n",
    "    r=16, lora_alpha=32, lora_dropout=0.05,\n",
    "    task_type=TaskType.CAUSAL_LM,\n",
    "    target_modules=[\"q_proj\",\"k_proj\",\"v_proj\",\"o_proj\",\"gate_proj\",\"up_proj\",\"down_proj\"]\n",
    ")\n",
    "model = get_peft_model(base, peft_cfg)\n",
    "print(\"Trainable params:\", sum(p.numel() for p in model.parameters() if p.requires_grad))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import DataCollatorForSeq2Seq\n",
    "\n",
    "data_collator = DataCollatorForSeq2Seq(\n",
    "    tokenizer=tokenizer,\n",
    "    model=None,\n",
    "    padding=True,\n",
    "    label_pad_token_id=-100,\n",
    "    return_tensors=\"pt\",\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import TrainingArguments\n",
    "\n",
    "bf16_ok = torch.cuda.is_available() and torch.cuda.get_device_capability(0)[0] >= 8\n",
    "\n",
    "args = TrainingArguments(\n",
    "    output_dir=\"outputs/llama_spider_lora\",\n",
    "    per_device_train_batch_size=1,\n",
    "    gradient_accumulation_steps=16,   # effective batch 16\n",
    "    learning_rate=2e-4,\n",
    "    num_train_epochs=3,\n",
    "    lr_scheduler_type=\"cosine\",\n",
    "    warmup_ratio=0.03,\n",
    "    logging_steps=25,\n",
    "    save_steps=500,\n",
    "    eval_steps=500,\n",
    "    save_total_limit=2,\n",
    "    bf16=bf16_ok,\n",
    "    fp16=(torch.cuda.is_available() and not bf16_ok),\n",
    "    report_to=\"none\"\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_8667/1831611660.py:3: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.\n",
      "  trainer = Trainer(\n",
      "The tokenizer has new PAD/BOS/EOS tokens that differ from the model config and generation config. The model config and generation config were aligned accordingly, being updated with the tokenizer's values. Updated tokens: {'pad_token_id': 128001}.\n",
      "/home/zeus/miniconda3/envs/cloudspace/lib/python3.12/site-packages/torch/_dynamo/eval_frame.py:929: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.5 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
      "  return fn(*args, **kwargs)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='564' max='564' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [564/564 2:28:07, Epoch 3/3]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>25</td>\n",
       "      <td>0.691100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>50</td>\n",
       "      <td>0.294000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>75</td>\n",
       "      <td>0.238500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>100</td>\n",
       "      <td>0.243700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>125</td>\n",
       "      <td>0.204000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>150</td>\n",
       "      <td>0.203300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>175</td>\n",
       "      <td>0.173600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>200</td>\n",
       "      <td>0.138800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>225</td>\n",
       "      <td>0.113000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>250</td>\n",
       "      <td>0.114300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>275</td>\n",
       "      <td>0.103000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>300</td>\n",
       "      <td>0.106200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>325</td>\n",
       "      <td>0.108500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>350</td>\n",
       "      <td>0.089700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>375</td>\n",
       "      <td>0.090700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>400</td>\n",
       "      <td>0.048200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>425</td>\n",
       "      <td>0.048300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>450</td>\n",
       "      <td>0.053000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>475</td>\n",
       "      <td>0.044700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>500</td>\n",
       "      <td>0.045700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>525</td>\n",
       "      <td>0.045900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>550</td>\n",
       "      <td>0.044300</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/zeus/miniconda3/envs/cloudspace/lib/python3.12/site-packages/torch/_dynamo/eval_frame.py:929: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.5 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
      "  return fn(*args, **kwargs)\n"
     ]
    }
   ],
   "source": [
    "from transformers import Trainer\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=args,\n",
    "    tokenizer=tokenizer,\n",
    "    data_collator=data_collator,\n",
    "    train_dataset=tok_ds[\"train\"],\n",
    "    eval_dataset=tok_ds[\"dev\"]\n",
    ")\n",
    "\n",
    "# fast sanity slice (adjust sizes as you like)\n",
    "small_train = tok_ds[\"train\"].shuffle(seed=42).select(range(3000))\n",
    "small_dev   = tok_ds[\"dev\"].shuffle(seed=42).select(range(500))\n",
    "trainer.train_dataset = small_train\n",
    "trainer.eval_dataset  = small_dev\n",
    "\n",
    "trainer.train()\n",
    "\n",
    "model.save_pretrained(\"outputs/llama_spider_lora/adapters\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/100 [00:00<?, ?it/s]Caching is incompatible with gradient checkpointing in LlamaDecoderLayer. Setting `past_key_values=None`.\n",
      "/home/zeus/miniconda3/envs/cloudspace/lib/python3.12/site-packages/torch/_dynamo/eval_frame.py:929: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.5 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
      "  return fn(*args, **kwargs)\n",
      "/home/zeus/miniconda3/envs/cloudspace/lib/python3.12/site-packages/torch/utils/checkpoint.py:85: UserWarning: None of the inputs have requires_grad=True. Gradients will be None\n",
      "  warnings.warn(\n",
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 100/100 [47:19<00:00, 28.39s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EM (subset): 0.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "GEN_KW = dict(max_new_tokens=256, temperature=0.001, top_p=1.0, pad_token_id=tokenizer.eos_token_id)\n",
    "\n",
    "@torch.inference_mode()\n",
    "def generate_sql(prompt:str):\n",
    "    inputs = tokenizer(prompt, return_tensors=\"pt\").to(model.device)\n",
    "    out = model.generate(**inputs, **GEN_KW)\n",
    "    text = tokenizer.decode(out[0], skip_special_tokens=True)\n",
    "    return text[len(prompt):].strip()\n",
    "\n",
    "def normalize_sql(s: str) -> str:\n",
    "    try:\n",
    "        return sqlglot.transpile(s, read=\"mysql\", write=\"mysql\", normalize=True, pretty=False)[0]\n",
    "    except Exception:\n",
    "        return s.strip()\n",
    "\n",
    "def exact_match(pred: str, gold: str) -> bool:\n",
    "    return normalize_sql(pred) == normalize_sql(gold)\n",
    "\n",
    "subset = ds[\"dev\"].select(range(100))\n",
    "hits=0\n",
    "for ex in tqdm(subset):\n",
    "    pred = generate_sql(ex[\"prompt\"])\n",
    "    hits += exact_match(pred, ex[\"sql_gold\"])\n",
    "print(\"EM (subset):\", hits/len(subset))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved LoRA adapters to: outputs/llama_spider_lora/adapters\n"
     ]
    }
   ],
   "source": [
    "save_dir = Path(\"outputs/llama_spider_lora/adapters\")\n",
    "save_dir.mkdir(parents=True, exist_ok=True)\n",
    "model.save_pretrained(save_dir.as_posix())\n",
    "print(\"Saved LoRA adapters to:\", save_dir)\n",
    "\n",
    "# Optional: push to HF Hub (adapters only)\n",
    "# from huggingface_hub import create_repo\n",
    "# create_repo(\"your-username/llama-spider-lora\", private=True, exist_ok=True)\n",
    "# model.push_to_hub(\"your-username/llama-spider-lora\")\n",
    "# tokenizer.push_to_hub(\"your-username/llama-spider-lora\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The `load_in_4bit` and `load_in_8bit` arguments are deprecated and will be removed in the future versions. Please, pass a `BitsAndBytesConfig` object in `quantization_config` argument instead.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c8744e7104504fa4819ee3b4fc8a9dfa",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "PeftModelForCausalLM(\n",
       "  (base_model): LoraModel(\n",
       "    (model): LlamaForCausalLM(\n",
       "      (model): LlamaModel(\n",
       "        (embed_tokens): Embedding(128256, 3072)\n",
       "        (layers): ModuleList(\n",
       "          (0-27): 28 x LlamaDecoderLayer(\n",
       "            (self_attn): LlamaAttention(\n",
       "              (q_proj): lora.Linear4bit(\n",
       "                (base_layer): Linear4bit(in_features=3072, out_features=3072, bias=False)\n",
       "                (lora_dropout): ModuleDict(\n",
       "                  (default): Dropout(p=0.05, inplace=False)\n",
       "                )\n",
       "                (lora_A): ModuleDict(\n",
       "                  (default): Linear(in_features=3072, out_features=16, bias=False)\n",
       "                )\n",
       "                (lora_B): ModuleDict(\n",
       "                  (default): Linear(in_features=16, out_features=3072, bias=False)\n",
       "                )\n",
       "                (lora_embedding_A): ParameterDict()\n",
       "                (lora_embedding_B): ParameterDict()\n",
       "                (lora_magnitude_vector): ModuleDict()\n",
       "              )\n",
       "              (k_proj): lora.Linear4bit(\n",
       "                (base_layer): Linear4bit(in_features=3072, out_features=1024, bias=False)\n",
       "                (lora_dropout): ModuleDict(\n",
       "                  (default): Dropout(p=0.05, inplace=False)\n",
       "                )\n",
       "                (lora_A): ModuleDict(\n",
       "                  (default): Linear(in_features=3072, out_features=16, bias=False)\n",
       "                )\n",
       "                (lora_B): ModuleDict(\n",
       "                  (default): Linear(in_features=16, out_features=1024, bias=False)\n",
       "                )\n",
       "                (lora_embedding_A): ParameterDict()\n",
       "                (lora_embedding_B): ParameterDict()\n",
       "                (lora_magnitude_vector): ModuleDict()\n",
       "              )\n",
       "              (v_proj): lora.Linear4bit(\n",
       "                (base_layer): Linear4bit(in_features=3072, out_features=1024, bias=False)\n",
       "                (lora_dropout): ModuleDict(\n",
       "                  (default): Dropout(p=0.05, inplace=False)\n",
       "                )\n",
       "                (lora_A): ModuleDict(\n",
       "                  (default): Linear(in_features=3072, out_features=16, bias=False)\n",
       "                )\n",
       "                (lora_B): ModuleDict(\n",
       "                  (default): Linear(in_features=16, out_features=1024, bias=False)\n",
       "                )\n",
       "                (lora_embedding_A): ParameterDict()\n",
       "                (lora_embedding_B): ParameterDict()\n",
       "                (lora_magnitude_vector): ModuleDict()\n",
       "              )\n",
       "              (o_proj): lora.Linear4bit(\n",
       "                (base_layer): Linear4bit(in_features=3072, out_features=3072, bias=False)\n",
       "                (lora_dropout): ModuleDict(\n",
       "                  (default): Dropout(p=0.05, inplace=False)\n",
       "                )\n",
       "                (lora_A): ModuleDict(\n",
       "                  (default): Linear(in_features=3072, out_features=16, bias=False)\n",
       "                )\n",
       "                (lora_B): ModuleDict(\n",
       "                  (default): Linear(in_features=16, out_features=3072, bias=False)\n",
       "                )\n",
       "                (lora_embedding_A): ParameterDict()\n",
       "                (lora_embedding_B): ParameterDict()\n",
       "                (lora_magnitude_vector): ModuleDict()\n",
       "              )\n",
       "            )\n",
       "            (mlp): LlamaMLP(\n",
       "              (gate_proj): lora.Linear4bit(\n",
       "                (base_layer): Linear4bit(in_features=3072, out_features=8192, bias=False)\n",
       "                (lora_dropout): ModuleDict(\n",
       "                  (default): Dropout(p=0.05, inplace=False)\n",
       "                )\n",
       "                (lora_A): ModuleDict(\n",
       "                  (default): Linear(in_features=3072, out_features=16, bias=False)\n",
       "                )\n",
       "                (lora_B): ModuleDict(\n",
       "                  (default): Linear(in_features=16, out_features=8192, bias=False)\n",
       "                )\n",
       "                (lora_embedding_A): ParameterDict()\n",
       "                (lora_embedding_B): ParameterDict()\n",
       "                (lora_magnitude_vector): ModuleDict()\n",
       "              )\n",
       "              (up_proj): lora.Linear4bit(\n",
       "                (base_layer): Linear4bit(in_features=3072, out_features=8192, bias=False)\n",
       "                (lora_dropout): ModuleDict(\n",
       "                  (default): Dropout(p=0.05, inplace=False)\n",
       "                )\n",
       "                (lora_A): ModuleDict(\n",
       "                  (default): Linear(in_features=3072, out_features=16, bias=False)\n",
       "                )\n",
       "                (lora_B): ModuleDict(\n",
       "                  (default): Linear(in_features=16, out_features=8192, bias=False)\n",
       "                )\n",
       "                (lora_embedding_A): ParameterDict()\n",
       "                (lora_embedding_B): ParameterDict()\n",
       "                (lora_magnitude_vector): ModuleDict()\n",
       "              )\n",
       "              (down_proj): lora.Linear4bit(\n",
       "                (base_layer): Linear4bit(in_features=8192, out_features=3072, bias=False)\n",
       "                (lora_dropout): ModuleDict(\n",
       "                  (default): Dropout(p=0.05, inplace=False)\n",
       "                )\n",
       "                (lora_A): ModuleDict(\n",
       "                  (default): Linear(in_features=8192, out_features=16, bias=False)\n",
       "                )\n",
       "                (lora_B): ModuleDict(\n",
       "                  (default): Linear(in_features=16, out_features=3072, bias=False)\n",
       "                )\n",
       "                (lora_embedding_A): ParameterDict()\n",
       "                (lora_embedding_B): ParameterDict()\n",
       "                (lora_magnitude_vector): ModuleDict()\n",
       "              )\n",
       "              (act_fn): SiLUActivation()\n",
       "            )\n",
       "            (input_layernorm): LlamaRMSNorm((3072,), eps=1e-05)\n",
       "            (post_attention_layernorm): LlamaRMSNorm((3072,), eps=1e-05)\n",
       "          )\n",
       "        )\n",
       "        (norm): LlamaRMSNorm((3072,), eps=1e-05)\n",
       "        (rotary_emb): LlamaRotaryEmbedding()\n",
       "      )\n",
       "      (lm_head): Linear(in_features=3072, out_features=128256, bias=False)\n",
       "    )\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from pathlib import Path\n",
    "import torch, bitsandbytes as bnb\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "from peft import PeftModel\n",
    "\n",
    "BASE_MODEL  = \"meta-llama/Llama-3.2-3B\"        # EXACT model you trained on\n",
    "ADAPTER_DIR = Path(\"outputs/llama_spider_lora/adapters\")  # or checkpoint-564\n",
    "\n",
    "# Tokenizer\n",
    "tok = AutoTokenizer.from_pretrained(BASE_MODEL, use_fast=True)\n",
    "if tok.pad_token is None:\n",
    "    tok.pad_token = tok.eos_token\n",
    "\n",
    "# Base in 4-bit if CUDA; CPU fallback loads in full precision (slow)\n",
    "base = AutoModelForCausalLM.from_pretrained(\n",
    "    BASE_MODEL,\n",
    "    device_map=\"auto\" if torch.cuda.is_available() else None,\n",
    "    load_in_4bit=True if torch.cuda.is_available() else False,\n",
    "    torch_dtype=\"auto\",\n",
    "    trust_remote_code=False,\n",
    ")\n",
    "\n",
    "# ðŸ” Attach LoRA adapters\n",
    "model = PeftModel.from_pretrained(base, ADAPTER_DIR.as_posix())\n",
    "model.eval()\n",
    "\n",
    "# (Optional) merge for pure base weights inference (not needed now)\n",
    "# model = model.merge_and_unload()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Active adapters: default\n",
      "Found LoRA params: True\n",
      "First layer q_proj: lora.Linear4bit(\n",
      "  (base_layer): Linear4bit(in_features=3072, out_features=3072, bias=False)\n",
      "  (lora_dropout): ModuleDict(\n",
      "    (default): Dropout(p=0.05, inplace=False)\n",
      "  )\n",
      "  (lora_A): ModuleDict(\n",
      "    (default): Linear(in_features=3072, out_features=16, bias=False)\n",
      "  )\n",
      "  (lora_B): ModuleDict(\n",
      "    (default): Linear(in_features=16, out_features=3072, bias=False)\n",
      "  )\n",
      "  (lora_embedding_A): ParameterDict()\n",
      "  (lora_embedding_B): ParameterDict()\n",
      "  (lora_magnitude_vector): ModuleDict()\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "# a) Check PEFT config/adapters\n",
    "print(\"Active adapters:\", getattr(model, \"active_adapter\", None))\n",
    "\n",
    "# b) Make sure LoRA layers exist in the module names\n",
    "has_lora = any(\"lora_A\" in n or \"lora_B\" in n for n, _ in model.named_parameters())\n",
    "print(\"Found LoRA params:\", has_lora)\n",
    "\n",
    "# c) Inspect one layer to see the injected LoRA\n",
    "first_q = dict(model.named_modules()).get(\"base_model.model.model.layers.0.self_attn.q_proj\", None)\n",
    "print(\"First layer q_proj:\", first_q)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "BASE: SELECT count(*) FROM student; SELECT count(*) FROM student; SELECT count(*) FROM student; SELECT count(*) FROM student; SELECT count(*) FROM student; SELECT count(*) FROM student; SELECT count(*) FROM\n",
      "PEFT: SELECT count(*) FROM student; SELECT count(*) FROM student; SELECT count(*) FROM student; SELECT count(*) FROM student; SELECT count(*) FROM student; SELECT count(*) FROM student; SELECT count(*) FROM\n"
     ]
    }
   ],
   "source": [
    "prompt = \"SYSTEM: Output ONLY SQL.\\nUSER:\\nQuestion: How many students are in the database?\\nSchema:\\nDatabase: academic\\nTables:\\n  student(id, *student_id, name)\\n\\nRules:\\n- Only use tables/columns from academic.\\n- Output only SQL, no commentary.\\n\"\n",
    "\n",
    "inputs = tok(prompt, return_tensors=\"pt\").to(model.device if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "with torch.inference_mode():\n",
    "    out_base = base.generate(**inputs, max_new_tokens=64, temperature=0.001, top_p=1.0, pad_token_id=tok.eos_token_id)\n",
    "    out_peft = model.generate(**inputs, max_new_tokens=64, temperature=0.001, top_p=1.0, pad_token_id=tok.eos_token_id)\n",
    "\n",
    "print(\"\\nBASE:\", tok.decode(out_base[0], skip_special_tokens=True)[len(prompt):].strip()[:200])\n",
    "print(\"PEFT:\", tok.decode(out_peft[0], skip_special_tokens=True)[len(prompt):].strip()[:200])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# EVAL-1: paths + helpers (keeps your existing model/tok)\n",
    "from pathlib import Path\n",
    "import re, sqlite3, pandas as pd, sqlglot, torch\n",
    "from datasets import load_dataset\n",
    "from transformers import StoppingCriteria, StoppingCriteriaList\n",
    "\n",
    "# ---- paths ----\n",
    "PREPARED_DIR = Path(\"prepared\")\n",
    "DEV_JSONL    = PREPARED_DIR / \"spider_dev.jsonl\"\n",
    "DB_ROOT      = Path(\"database\")          # you already copied these to Lightning\n",
    "OUT_CSV      = Path(\"outputs/dev_eval.csv\")\n",
    "\n",
    "# ---- (re)load dev if not in memory ----\n",
    "if \"ds\" not in globals() or \"dev\" not in ds:\n",
    "    ds = load_dataset(\"json\", data_files={\"dev\": str(DEV_JSONL)})\n",
    "dev_ds = ds[\"dev\"]\n",
    "\n",
    "# ---- decoding: stop exactly at first ';' ----\n",
    "class StopAtSemicolon(StoppingCriteria):\n",
    "    def __init__(self, tokenizer, prompt_len): self.tok = tokenizer; self.n = prompt_len\n",
    "    def __call__(self, input_ids, scores, **kwargs):\n",
    "        txt = self.tok.decode(input_ids[0][self.n:], skip_special_tokens=True)\n",
    "        return \";\" in txt\n",
    "\n",
    "GEN = dict(\n",
    "    max_new_tokens=256,\n",
    "    temperature=0.001,\n",
    "    top_p=1.0,\n",
    "    pad_token_id=tok.eos_token_id,\n",
    "    no_repeat_ngram_size=3,\n",
    "    repetition_penalty=1.05\n",
    ")\n",
    "\n",
    "def robust_suffix(full: str, prompt: str) -> str:\n",
    "    i = full.find(prompt)\n",
    "    return full[i+len(prompt):].strip() if i != -1 else full.strip()\n",
    "\n",
    "SQL_START = re.compile(r\"(?is)\\b(SELECT|WITH|INSERT|UPDATE|DELETE)\\b\")\n",
    "SQL_SPAN  = re.compile(r\"(?is)\\b(SELECT|WITH|INSERT|UPDATE|DELETE)\\b.*?;\")\n",
    "\n",
    "def extract_sql(text: str) -> str:\n",
    "    m = SQL_SPAN.search(text)\n",
    "    if m: return m.group(0).strip()\n",
    "    m = SQL_START.search(text)\n",
    "    return text[m.start():].strip() if m else text.strip()\n",
    "\n",
    "@torch.inference_mode()\n",
    "def generate_sql(prompt: str) -> str:\n",
    "    inputs = tok(prompt, return_tensors=\"pt\").to(model.device if torch.cuda.is_available() else \"cpu\")\n",
    "    stopper = StoppingCriteriaList([StopAtSemicolon(tok, inputs[\"input_ids\"].shape[1])])\n",
    "    out = model.generate(**inputs, stopping_criteria=stopper, **GEN)\n",
    "    full = tok.decode(out[0], skip_special_tokens=True)\n",
    "    return extract_sql(robust_suffix(full, prompt))\n",
    "\n",
    "def normalize_sql(s: str) -> str:\n",
    "    try:\n",
    "        return sqlglot.transpile(s, read=\"mysql\", write=\"mysql\", normalize=True, pretty=False)[0]\n",
    "    except Exception:\n",
    "        return s.strip()\n",
    "\n",
    "def exact_match(pred: str, gold: str) -> bool:\n",
    "    return normalize_sql(pred) == normalize_sql(gold)\n",
    "\n",
    "def run_sql(db_id: str, sql: str):\n",
    "    dbp = DB_ROOT / db_id / f\"{db_id}.sqlite\"\n",
    "    if not dbp.exists(): return False, f\"db_missing:{dbp}\"\n",
    "    try:\n",
    "        con = sqlite3.connect(dbp.as_posix())\n",
    "        df = pd.read_sql_query(sql, con); con.close()\n",
    "        if len(df.columns)>0: df = df.sort_values(list(df.columns)).reset_index(drop=True)\n",
    "        return True, df\n",
    "    except Exception as e:\n",
    "        return False, str(e)\n",
    "\n",
    "def exec_match(pred_sql: str, gold_sql: str, db_id: str) -> bool:\n",
    "    okp, rp = run_sql(db_id, pred_sql)\n",
    "    okg, rg = run_sql(db_id, gold_sql)\n",
    "    if not (okp and okg): return False\n",
    "    try: return rp.equals(rg)\n",
    "    except: return False\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 100/100 [06:57<00:00,  4.17s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "EM: 0.300 | EX: 0.480 | N=100\n",
      "Saved: outputs/dev_eval.csv\n",
      "\n",
      "Sample misses:\n",
      "--------------------------------------------------------------------------------\n",
      "DB: concert_singer\n",
      "Q : How many singers do we have?\n",
      "P : SELECT count(*) FROM singer SELECT count(*),  name FROM singer GROUP BY name ORDER BY count(*) DESC LIMIT 1;\n",
      "G : SELECT count(*) FROM singer\n",
      "--------------------------------------------------------------------------------\n",
      "DB: concert_singer\n",
      "Q : How many singers are from each country?\n",
      "P : SELECT count(*),  country FROM singer GROUP BY country;\n",
      "G : SELECT country ,  count(*) FROM singer GROUP BY country\n",
      "--------------------------------------------------------------------------------\n",
      "DB: concert_singer\n",
      "Q : List all song names by singers above the average age.\n",
      "P : SELECT Song_Name FROM singer WHERE Age  >  (SELECT avg(Age) FROM singer) ORDER BY Age ASC;\n",
      "G : SELECT song_name FROM singer WHERE age  >  (SELECT avg(age) FROM singer)\n",
      "--------------------------------------------------------------------------------\n",
      "DB: concert_singer\n",
      "Q : What are all the song names by singers who are older than average?\n",
      "P : SELECT Song_Name FROM singer WHERE Age  >  (SELECT avg(Age) FROM singer) ORDER BY Song_Name ASC;\n",
      "G : SELECT song_name FROM singer WHERE age  >  (SELECT avg(age) FROM singer)\n",
      "--------------------------------------------------------------------------------\n",
      "DB: concert_singer\n",
      "Q : Show location and name for all stadiums with a capacity between 5000 and 10000.\n",
      "P : SELECT LOCATION,  NAME FROM stadium WHERE capacity BETWEEN 500 AND 1000;\n",
      "G : SELECT LOCATION ,  name FROM stadium WHERE capacity BETWEEN 5000 AND 10000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "from tqdm import tqdm\n",
    "import pandas as pd\n",
    "\n",
    "EVAL_LIMIT = 100          # set to None for full dev\n",
    "subset = dev_ds if EVAL_LIMIT is None else dev_ds.select(range(min(EVAL_LIMIT, len(dev_ds))))\n",
    "\n",
    "rows = []\n",
    "em_hits = ex_hits = 0\n",
    "for ex in tqdm(subset, desc=\"Evaluating\"):\n",
    "    pred = generate_sql(ex[\"prompt\"])\n",
    "    em_ok = exact_match(pred, ex[\"sql_gold\"])\n",
    "    ex_ok = exec_match(pred, ex[\"sql_gold\"], ex[\"db_id\"])\n",
    "    em_hits += int(em_ok); ex_hits += int(ex_ok)\n",
    "    rows.append({\n",
    "        \"db_id\": ex[\"db_id\"],\n",
    "        \"question\": ex[\"question\"],\n",
    "        \"pred_sql\": pred,\n",
    "        \"gold_sql\": ex[\"sql_gold\"],\n",
    "        \"em\": int(em_ok),\n",
    "        \"ex\": int(ex_ok),\n",
    "    })\n",
    "\n",
    "EM = em_hits / len(subset)\n",
    "EX = ex_hits / len(subset)\n",
    "print(f\"\\nEM: {EM:.3f} | EX: {EX:.3f} | N={len(subset)}\")\n",
    "\n",
    "df = pd.DataFrame(rows)\n",
    "OUT_CSV.parent.mkdir(parents=True, exist_ok=True)\n",
    "df.to_csv(OUT_CSV, index=False)\n",
    "print(\"Saved:\", OUT_CSV)\n",
    "\n",
    "# show a few misses for debugging\n",
    "print(\"\\nSample misses:\")\n",
    "for _, r in df[(df.em==0) | (df.ex==0)].head(5).iterrows():\n",
    "    print(\"-\"*80)\n",
    "    print(\"DB:\", r.db_id)\n",
    "    print(\"Q :\", r.question)\n",
    "    print(\"P :\", r.pred_sql)\n",
    "    print(\"G :\", r.gold_sql)\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
